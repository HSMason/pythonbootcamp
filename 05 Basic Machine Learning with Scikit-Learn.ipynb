{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Machine Learning with Scikit-Learn\n",
    "\n",
    "In this notebook we will look into the basics of building ML models with Scikit-Learn. Scikit-Learn is the most widely used Python libray for ML, especially outside of deep learning (where there are several contenders and I recommend using Keras, which is a package that provides a simple API on top of several underlying contenders like TensorFlow and PyTorch). Once again, this notebook draw heavily on Jake Vanderplas's excellent Python Data Science handbook.\n",
    "\n",
    "We'll proceed in this fashion:\n",
    "\n",
    "- give a brief overview of key terminology\n",
    "- illustrate the typical use of SciKit-Learn API through some simple examples\n",
    "- discuss various metrics that can be used to evaluate ML models\n",
    "- dive deeper with some more complex examples\n",
    "- look at the various ways we can validate and improve our models\n",
    "- discuss the topic of feature engineering - ML models are good examples of \"garbage in, garbage out\", so cleaning our data and getting the right features is important\n",
    "- finally, summarize some of the main model techniques and their pros and cons\n",
    "\n",
    "\n",
    "## ML Terminology\n",
    "\n",
    "*Machine Learning* can be considered the process of building models from data, either to gain insight into the data or to make predictions from the data. There are two main categories:\n",
    "\n",
    "- *supervised learning*, in which the data is labelled with an outcome, and the aim is to predict new outcomes from new data; in the case where the outcome is a category this is called *classification* while if the outcome is a continuous quantity this is *regression*\n",
    "- *unsupervised learning* where the data is analyzed for some underlying patterns to gain insight; common examples are *clustering* (finding similar cases) and *dimensionality reduction* (reducing the number of variables needed to represent the data, essentially a form of lossy compression)\n",
    "\n",
    "The data used in ML is typically tabular. The columns are called *features*. For supervised learning, we call the  output the *label*. We'll often refer to the vector if features as X and the output (label) as y, and say that we're trying to find a function that approximates f(X)=y; this function is our *model* and is characterized by some *model parameters*. We usually choose the type or class of model, and then use ML techniques to learn the model parameters that minimize the *error* (the difference between the predicted and actual output). More generally we can think of this as an optimization problem, where we are trying to learn the parameters that minimize a *loss function*; that loss function is typically going to be some cummulative function of the errors; a common loss function is RMSE (root mean square of errors).\n",
    "\n",
    "While we learn the model parameters, there are some parameters we need to specify as inputs to the model too; these are called *hyperparameters*. For example, if we learn a decision tree model, the parameters might be the features being tested at each branch and the values they are being tested against, while the hyperparameters would include the depth that we want to limit the tree to. For a polynomial regression model, the parameters would be the coefficients of the polynomial, while the hyperparameters could include the degree of polynomial we want to learn.\n",
    "\n",
    "In this notebook we are going to focus on practical ML. We will not be going into the details of the various algorithms. We're also going to avoid the topics of *Deep Learning* and *neural nets*: these are generating a lot of hype at present but in general involve learning thousands of parameters requiring massive amounts of data for training (and a lot of time) and so are more limited in the domains in which they can be applied. The techniques we are going to look at can be used for much smaller problems and are generally very fast.\n",
    "\n",
    "> It's worth noting that getting good data and the right features is generally more important than the type of model \n",
    "> you choose to use. The performance of the different types of ML model techniques is often similar. It's commonly\n",
    "> said in ML circles that 80% of time is spent on getting data cleaned up and doing *feature engineering* and only 20%\n",
    "> is spent on modeling. For example, until around 2014 when deep learning started taking off, most ML models at Google \n",
    "> were trained using the same system (Sibyl), a form of boosted logistic regression.\n",
    "\n",
    "\n",
    "## Exploring the Iris Dataset\n",
    "\n",
    "We would usually start with some exploratory analysis of the data. A very common approach if the number of features is not too high is to use a *pairplot* or *scatter matrix*. We'll use the famous Iris dataset in this example. We'll eventually be trying to predict the species, so it will be a focus even in our initial exploratory work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.pairplot(iris, hue='species', size=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pairplot, we're looking for several things: \n",
    "\n",
    "- are there features that are strongly correlated (or strongly inversely correlated)? If so, we may want to exclude one of the features, to simplify the model;\n",
    "- are there obvious clusters, that may be linearly separable? If so we may get far with simple linear models (as opposed to more complex models like decision trees).\n",
    "- are there features which seem correlated but our intuition or domain knowledge tells us should not be? In this case we may be missing features that are important to the model (these are called *confounding variables*)\n",
    "\n",
    "For an example, we may see a correlation between daily ice cream consumption and drive-by shooting rates. We shouldn't expect this. It may be that the confounding variable is weather - in hotter weather people are more likely to be outside, resulting in an increase in both ice cream consumption and drive-by shootings.\n",
    "\n",
    "If we are looking at classification, we should aim to have a dataset that is balanced in the number of observations of each category. We can check this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(iris.groupby('species').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset is not balanced we may want to resample it, over-sampling from items from less common classes or undersampling from items with more common classes.\n",
    "\n",
    "It's worth paying attention to the histograms on the diagonal of the scatter plot. Some modeling techniques work best when these features have Gaussian distributions. Another way of looking at these is with density plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "iris.plot(kind='density', subplots=True, sharex=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the distributions are not Gaussian, we may want to normalize them with a Box Cox transformation. That is beyond the scope of this notebook; Google it if you need it. Some ML models depend on the inputs being Gaussian; e.g. logistic regression, and Linear Discriminant Analysis (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Classification Model to Predict the Species\n",
    "\n",
    "In practice, we would usually store the data in the form of a Pandas DataFrame or NumPy array, the *features matrix*, and assign it to a variable `X`. Lets get the data into the X/y form we need for training a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.drop('species', axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = iris['species'] \n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit-Learn has a simple and consistent API that makes it easy to learn. After getting the data in the right form, the steps are just:\n",
    "\n",
    "- import the model type\n",
    "- instantiate the model with the appropriate *hyperparameters*\n",
    "- call the `fit()` API to learn the model parameters\n",
    "- call `predict()` to predict the results for new data\n",
    "\n",
    "We want to test our model on new data; the typical way to do this is to split the data into a *training set* and a *testing set*; a common split is 80/20 or 75/25.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "        \n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=1, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model type we will apply is Gaussian naive Bayes; this is simple and fast and needs no hyperparameters so it is a good baseline. It does work best if the inputs have Gaussian distributions but we'll ignore that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "        \n",
    "model = GaussianNB()\n",
    "model.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypredict = model.predict(Xtest)\n",
    "ypredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model we want to know the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "        \n",
    "accuracy_score(ytest, ypredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even a naive model is 97% accurate on this data.\n",
    "\n",
    "Let's look at a different model, so you can see how easy it is to switch between different approaches. We'll try a decision tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=4)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypredict = model.predict(Xtest)\n",
    "accuracy_score(ytest, ypredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see how well we have done is to use a *confusion matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "mat = confusion_matrix(ytest, ypredict)\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that we got 29 predictions right, and only one wrong.\n",
    "\n",
    "Confusion matrices are particularly useful for binary classifiers (only two output states), where we can clearly see the number of false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning - Dimensionality Reduction with PCA on Iris Dataset\n",
    "\n",
    "We'll now look at an example of unsupervised learning, doing Principal Component Analysis to do dimensionality reduction on the dataset so we can reduce to two dimensions for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "model = PCA(n_components=2) # Reduce to two features/dimensions\n",
    "model.fit(X)  # No need to specify y for unsupervised.\n",
    "X_2D = model.transform(X) \n",
    "# See the first few entries\n",
    "X_2D[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets make a DataFrame and plot it.\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_2D, columns=['PCA1', 'PCA2'])\n",
    "df['species'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(\"PCA1\", \"PCA2\", hue='species', data=df, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing to note is how the species are well separated. But when we generated the PCA values we did not tell the model anything about the species! This means that we should be able to use the input features (or the PCA features) and generate a good classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning - Clustering on Iris Dataset\n",
    "\n",
    "Now let's look at clustering. We'll use k-means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X)\n",
    "y_kmeans = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how well we have done by plotting each cluster separately. We'll use the PCA pseudo-features so we can still use 2D plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cluster'] = y_kmeans\n",
    "sns.lmplot('PCA1', 'PCA2', data=df, hue='species', col='cluster', fit_reg=False); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear we have done very well in clustering in the blue case, but a bit less well with the other two species; this makes sense if you look at the earlier PCA plot, where you can see the blue species is more cleanly separated than the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Evaluating Supervised Models\n",
    "\n",
    "So far we have used classification accuracy, i.e. the percentage of correct predictions made, to evaluate models. This is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case. However, there are other metrics we can use. We'll briefly look at these here and then show how they can be used in the next couple of examples.\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "### Logarithmic Loss\n",
    "\n",
    "For multi-class classification where the model outputs a probability for each class we can use log-loss. This is commonly used in Kaggle competitions. It is defined as:\n",
    "\n",
    "   $LL = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^M y_{ij} \\log \\, p_{ij}$\n",
    "\n",
    "where $N$ is the number of samples or instances, $M$ is the number of possible labels, $y_{ij}$ is a binary indicator of whether or not label $j$ is the correct classification for instance $i$, and $p_{ij}$ is the model probability of assigning label $j$ to instance $i$. \n",
    "\n",
    "Smaller logloss is better with 0 representing a perfect classification.\n",
    "\n",
    "#### Classification Report\n",
    "\n",
    "This shows precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Precision* or *positive predictive value (PPV)* is the ratio of correct classifications (true positives) to total classifications for the class (i.e. TP / (TP + FP), where TP is # of true positives and FP is number of false postives)\n",
    "\n",
    "- *Recall* or *sensitivity* is the ratio of correct classifications to total instances of the class (i.e. TP / (TP + FN))\n",
    "\n",
    "- *F1* is the harmonic mean of precision and recall.\n",
    "\n",
    "You can read more about these measures here: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "#### Area under ROC Curve\n",
    "\n",
    "Area under ROC Curve (or AUC) represents a binary classifier’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random. ROC can be broken down into sensitivity (true positive rate/recall) and specificity (true negative rate), which are traded off.\n",
    "\n",
    "We're not going to go into the details here; if you're interested there is a deep discussion here: http://mlwiki.org/index.php/ROC_Analysis\n",
    "\n",
    "### Regression Metrics\n",
    "\n",
    "#### Mean Absolute Error\n",
    "\n",
    "This is just the sum of the absolute values of the prediction errors; i.e.:\n",
    "\n",
    "   $MAE = \\frac{\\sum\\limits_{i=1}^n |y_i - \\hat{y_i}|}{n}$\n",
    "\n",
    "where $y_i$ is the actual value for the $i$th sample and $\\hat{y_i}$ is the predicted value.\n",
    "\n",
    "\n",
    "#### Mean Squared Error\n",
    "\n",
    "This is the mean value of the square of the errors:\n",
    "\n",
    "   $MSE = \\frac{\\sum\\limits_{i=1}^n (y_i - \\hat{y_i})^2}{n}$\n",
    "\n",
    "To reduce the size of the value we can take the square root at the end; this gives us the Root Mean Squared Error or RMSE.\n",
    "\n",
    "#### $R^2$\n",
    "\n",
    "$r^2$ is also known as the coefficient of determination and is 0 for no fit and 1 for a perfect fit. It is the proportion of the variation in the dependent variable that is predictable from the independent variable(s). See https://en.wikipedia.org/wiki/Coefficient_of_determination for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "\n",
    "We've seen one way of doing model validation; namely, splitting the data into a training set and a test set. In this section we'll look at a more sophisticated approach, namely cross-validation. We'll also see how we can use run into the problem of overfitting the data, and discuss the two concepts of bias and variance, and how these are traded-off.\n",
    "\n",
    "### Overfitting the Training Data\n",
    "\n",
    "Imagine that for our model, we just hashed the input features to the output label. If we ran this as a model, and passed the same training data in as test data, we would get an exact match every time, suggesting we have a 100% accurate model! But if we gave the model new data it hadn't seen, it would fail dismally, not even being able to predicta result. We could address this by using some kind of fuzzy matching of hash keys, so at least we could get predictions out, but the result is likely to be a poor model. This illustrates two problems:\n",
    "\n",
    "- we shouldn't test with our training data or we are likely to get an unreasonable expectation of accuracy\n",
    "- the model may 'fit' the training data very well but not generalize well to new data; we call this 'overfitting'\n",
    "\n",
    "On the other hand, if we split into test and training data (where the test data is what we call a 'holdout set'), we lose some of our training data. What to do? Fortunately, there is a way we can use all of our data for training *and* for testing, using *cross-validation*!\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "Let's say we split the data in half; call the two halves H1 and H2. We could train a model on H1 and test it on H2, and get an accuracy A1. We could train a second model on H2 and test it on H1, and get an accuracy of A2. This suggests that if we train a model using all the data, the accuracy when tested on new data should be somewhere between A1 and A2.\n",
    "\n",
    "In practice, we don't split in half. More generally, we will split the data into k pieces of *folds*, $F_1$, $F_2$, ..., $F_k$, and then build $k$ models, where the $n$th model uses $F_n$ as the test set and the remaining pieces excluding $F_n$ as the training set. The $k$ individual scores are then combined ito a final score (mean and standard deviation). This is called *k-fold cross validation*. The size of each fold must  be large enough to be a reasonable sample of the problem, while allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on new data. Commonly-used values for k are 5 or 10, although we can go all the way to the extreme where $k$ is the size of the training set; this is called *leave-one-out cross validation*.  It is computationally expensive and the results have high variance.\n",
    "\n",
    "We can do k-fold cross-validation in SciKit-Learn easily with `cross_val_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)  # 5-fold; this will call model.fit() 5 times with different data\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bias-Variance Trade-Off\n",
    "\n",
    "What can we do if our model performs poorly? Assuming it is not just due to the data being random, in which case we may never be successful, we have several possible strategies:\n",
    "\n",
    "- use a more complex model to get a better fit to training data (reduce bias, increase variance)\n",
    "- use a simpler model because we may have overfit the training data (increase bias, reduce variance)\n",
    "- get more data to train on\n",
    "- add more features (we'll talk about this later)\n",
    "\n",
    "There are two measures mentioned above, bias and variance. Without getting too much into the details, bias is a measure of error (how far off predictions are from reality) while variance can be thought of as how closely the model fits to noise. So a model with high bias underfits, while a model with high variance overfits, and we want to find the sweet spot between these (best fit). A model with high variance will do poorly on the validation set (as it overfits the training set), while a model with high bias should do about as well on the validation set as the training set (or about as bad!)\n",
    "\n",
    "If we increased the complexity of our model, it would do better and better at fitting the training data, but at some point would start getting worse and worse at fitting the validation data. This is the *bias-variance trade-off*; you can see it in the image below where the vertical dashes line is the sweet spot we want to find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAHRCAYAAABgomDFAAAAAXNSR0ICQMB9xQAAAAlwSFlzAAAX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\nAAAAQPoAAADgn/8P8XVL2vO9ncEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "image/png": {
       "height": 400,
       "width": 400
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "url = 'https://i.stack.imgur.com/S0tRm.png'\n",
    "Image(url,width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "Note that the first two options we have, around model complexity, assume that the data is a fixed size. The third option is to add more data. As we add more data the optimal model will change. Assuming we capture the complexity of the data we are modelling, at some point we should have diminishing returns where adding more data doesn't significantly improve the model. This means that if we have a lot of data, it can be useful to meaure how well the model is doing as we are training it, so we can stop when we reach the point where the further improvements are negligible. If we plot the training and validation scores as a function of the size of the data set we get the *learning curve*.\n",
    "\n",
    "We would expect that a model will overfit a small dataset and underfit a large dataset, where 'small' and 'large' are dependent on the complexity of the model. We would also expect in almost every case that the training score is better than the validation score(or equivalently the training error is lower than the validation error). So a typical learning curve will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNQAAAExCAMAAACDJw8tAAAABGdBTUEAALGPC/xhBQAAACBjSFJN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\nMzBUMDY6Mzk6NDcrMDA6MDDkjs9eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "image/png": {
       "height": 800,
       "width": 800
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "url = 'https://cdn-enterprise.discourse.org/udacity/uploads/default/original/3X/1/e/1e28f112574e48fea279ddf1b052c0d3e774f470.png'\n",
    "Image(url,width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the curve converges to some score as the amount of data increases.\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "Drawing plots of validation and learning curves can be useful except that in many cases models have multiple knobs (hyperparameters) to adjust; this makes using such visual aids more difficult. Instead, we can just train multiple versions of the model and have SciKit-Learn do the work, using a *grid search*.\n",
    "\n",
    "To do this, we need to create a dictionary, where the keys are the different model constructor arguments, and the values are lists of values that we want to try. Then we can have SciKit-Learn find the best values for the parameters with cross-validation using `GridSearchCV`: \n",
    "\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "    grid = GridSearchCV(model, param_dict, cv=10)\n",
    "    grid.fit(X, y)\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    final_model = grid.best_estimator_\n",
    "\n",
    "It's also possible to use a list of dictionaries. SciKit-Learn will do an exhaustive search of the entire parameter space by default. `RandomizedSearchCV` can be used for a randomized search instead.\n",
    "\n",
    "You can read more here: http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Data Transforms\n",
    "\n",
    "Apart from changing the model complexity and adding more data, the main other strategy we have for improving our models is to add more features. We may be able to get more data from the problem domain, but in many cases we don't have that option. All is not lost though, as we can use other sources of data for dependent features, as well as sometimes synthesize new features from the features we are given. We will look at this in this section, along with ways of transforming our data to be more usable by our models.\n",
    "\n",
    "SciKit-Learn helps us out here too; it supports a rich set of data transforms. The general approach is: create a transform, *fit* it to the data, and then apply it to *transform* the data. When new data arrives the previously fit transform can be applied to that for consistency. For the training data fit and tranform can usually be combined into a call to `.fit_transform()`.\n",
    "\n",
    "### Rescaling Data to a Min/Max Range\n",
    "\n",
    "This can be done easily with:\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X2 = scaler.fit_transform(X)\n",
    "    \n",
    "It applies the transform $x' = \\frac{x - min(\\bf{x})}{max(\\bf{x}) - min(\\bf{x})}$.\n",
    "\n",
    "Rescaling to [0, 1] is the most common use case.\n",
    "\n",
    "\n",
    "### Standardizing Distributions\n",
    "\n",
    "To convert a Gaussian-distributed value to have mean 0 and standard deviation 1 we use z-score normalization:\n",
    "\n",
    "$x' = \\frac{x - E(\\bf{x})}{\\sqrt{Var(\\bf{x})}}$\n",
    "\n",
    "where $E(\\bf{x})$ is the expected value or mean of $\\bf{x}$ and the standard deviation is the square root of the variance of $\\bf{x}$.\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X2 = scaler.transform(X)\n",
    "\n",
    "### Normalizing to Unit Length Vectors\n",
    "\n",
    "Normalized data has each row/observation have length 1 when treated as an n-dimensional vector.\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    scaler = Normalizer().fit(X)\n",
    "    X2 = scaler.transform(X)\n",
    "\n",
    "### Binarizing/Thresholding\n",
    "\n",
    "    from sklearn.preprocessing import Binarizer\n",
    "\n",
    "    binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "    binaryX = binarizer.transform(X)\n",
    "\n",
    "### Handling Categorical Data\n",
    "\n",
    "Many ML algorithms rely on the inputs being numeric quantities. So how should we deal with categorical data? We could use a simple mapping of categories to numbers, but this is a bad idea, as the models assume that the values have some numerical significance.\n",
    "\n",
    "A better approach is to use *1-hot encoding*. We saw this briefly in the Pandas notebook. The idea is to add a new column for every unique value of that categorical variable, and then encode the values in those columns as zeroes except for the column corresponding to the original value.\n",
    "\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "        {'color': 'red'},\n",
    "        {'color': 'blue'},\n",
    "        {'color': 'green'},\n",
    "        {'color': 'blue'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "v = DictVectorizer(sparse=False, dtype=int)\n",
    "v.fit_transform(data)  # We could use .transform() but would need to set a 'vocabulary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a lot of different values for the category, this can add many columns, which may affect which modeling approaches we can use later. In this case you probably will want to set the `sparse` argument to `True` to save memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Text\n",
    "\n",
    "For text, there are a few different options. Most commonly we will split the text up into a sequence of words. In many cases we will remove *stop words* (common words like 'and' and 'the'), and may perform operations such as 'stemming', 'named entity recognition', 'parts of speech tagging', etc. We're not going to go into these here; see the SpaCy NLP library for examples (https://spacy.io/).\n",
    "\n",
    "We can use a similar approach to the categorical data, and turn each unique word into a new column. We can encode with 0/1 like one-hot, or use word counts. Let's look at the latter. In this case `fit_transform` returns a sparse array so we turn it back into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the mat belonged to the rat\",\n",
    "    \"the hat was on the mat\",\n",
    "    \"the cat ate the rat\",\n",
    "    \"the cat now has the hat\"\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer()\n",
    "X = v.fit_transform(data)\n",
    "pd.DataFrame(X.toarray(), columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with word counts, especially if we don't remove stop words, is that common words get scored highly, which may not always be desirable. What we want to score highly are words that are common *in this row* that are not common *across all rows*. That should weight words that are significant to the particular row. We can do this with *Term Frequency - Inverse Document Frequency* or TF-IDF scores. In our small example this doesn't quite have the desired effect but the code is useful to show:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "X = v.fit_transform(data)\n",
    "pd.DataFrame(X.toarray(), columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about TF-IDF here: https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "We saw a bit about handling missing data in the Pandas notebook. SciKit-Learn has some useful functionality to help us here too, namely the `Imputer`:\n",
    "\n",
    "    from sklearn.preprocessing import Imputer\n",
    "        \n",
    "    X = Imputer(strategy='mean').fit_transform(X)\n",
    "\n",
    "The above code will replace any `nan` values in the original array X with the mean of the non-nan values in that column. Other strategies are `median` and `most_frequent`.\n",
    "\n",
    "\n",
    "### Synthesizing and Adding Features\n",
    "\n",
    "In many cases, we will want to create features that are not directly available in the data, but are derived or synthesized from these. For example, the data may have city names, which we may want to turn into latitude/longitude, or perhaps include data like the population of the city. In such cases we would need to include the necessary tables to look up these features from the input features before fitting or predicting with the model.\n",
    "\n",
    "Another common case is to improve the power of regression. Regression fits a line in 2D, or a plane in higher dimensions, but this is linear. What if a polynomial of higher order would be a much better fit? We can still use linear regression, all we need to do is add features that are higher-order powers of the input features!\n",
    "\n",
    "Consider this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([4, 2, 1, 3, 7])\n",
    "plt.scatter(x, y);\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just use these features as is and fit a linear model we get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "X = x[:, np.newaxis]\n",
    "model = LinearRegression().fit(X, y)\n",
    "yfit = model.predict(X)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some synthetic features that are higher-order powers of X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "        \n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "X2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit a linear model to this extended set of features, and plot the results; you'll see we have a much better fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X2, y)\n",
    "        \n",
    "yfit = model.predict(X2)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Features\n",
    "\n",
    "Sometimes the right thing to do is to reduce the number of features. For example, we may have performance constraints (memory/CPU) that mean we need to shrink the model. Or we may have extraneous features that aren't adding any value.\n",
    "\n",
    "We've already seen one approach to this, namely PCA. There are other approaches too:\n",
    "\n",
    "- we can use decision tree models to rank the importance of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the Iris dataset again\n",
    "X = iris.drop('species', axis=1)\n",
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can use Recursive Feature Elimination (RFE) to recursively remove attributes and build a model based on those that remain, then use the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute. The example below uses logistic regression but any algorithm could be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use a chi-squared test to select the features with the strongest relationships with the output. The code below selects the three best features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "test = SelectKBest(score_func=chi2, k=3)\n",
    "fit = test.fit(X, y)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Sometimes we need to chain computations in SciKit. For example, when we looked at adding polynomial synthetic features, we produced a transformed feature vector before applying the linear model. This is a common enough operation that there is a mechanism available for it, namely pipelines.\n",
    "\n",
    "In that example, the code had a form similar to:\n",
    "\n",
    "````python\n",
    "X2 = PolynomialFeatures(degree=3, include_bias=False).fit_transform(X)\n",
    "model = LinearRegression().fit(X2, y)\n",
    "yfit = model.predict(trainy)\n",
    "````\n",
    "\n",
    "To write this as a pipeline, we could use:\n",
    "\n",
    "````python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(PolynomialFeatures(degree=3, include_bias=False), LinearRegression())\n",
    "model = pipeline.fit(X, y)\n",
    "yfit = model.predict(trainy)\n",
    "````\n",
    "\n",
    "Pipelines can help avoid a common mistake of leaking information from the test data set into the training data set. E.g. if we scale all data to be in range [0..1] but include the test data when determining the scale factors, this could be problematic. By doing the scaling in a pipeline along with model fitting this is less likely to occur. \n",
    "\n",
    "A more advanced feature of pipelines is the FeatureUnion. This is beyond the scope of this notebook but worth knowing about. See http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html.\n",
    "\n",
    "Using a regression model with polynomial synthetic features is a useful and common enough approach that we can define a utility function for this; we will use this later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Classification Example - Breast Cancer\n",
    "\n",
    "A description of this data is at http://scikit-learn.org/stable/datasets/index.html#datasets, section 5.14. The columns are mean, standard error, and \"worst\" values of the following ten features, making 30 features in all,\n",
    "followed by the class (malignant or benign).\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry\n",
    "- fractal dimension (“coastline approximation” - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "features = load_breast_cancer().feature_names\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=8)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypredict = model.predict(Xtest)\n",
    "accuracy_score(ytest, ypredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the most important features are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted(list(zip(features, model.feature_importances_)), key=itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Regression Example - Boston Housing Prices\n",
    "\n",
    "A description of this data is at http://scikit-learn.org/stable/datasets/index.html#datasets, section 5.13. The columns are:\n",
    "\n",
    "- CRIM per capita crime rate by town\n",
    "- ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS proportion of non-retail business acres per town\n",
    "- CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- NOX nitric oxides concentration (parts per 10 million)\n",
    "- RM average number of rooms per dwelling\n",
    "- AGE proportion of owner-occupied units built prior to 1940\n",
    "- DIS weighted distances to five Boston employment centres\n",
    "- RAD index of accessibility to radial highways\n",
    "- TAX full-value property-tax rate per \\$10,000\n",
    "- PTRATIO pupil-teacher ratio by town\n",
    "- B $1000(B_k - 0.63)^2$ where $B_k$ is the proportion of blacks by town\n",
    "- LSTAT \\% lower status of the population\n",
    "- MEDV Median value of owner-occupied homes in $1000’s\n",
    "\n",
    "MEDV is usually the target when building models with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "features = load_boston().feature_names\n",
    "X, y = load_boston(return_X_y=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a polynomial regression model and see how the model validation changes as we use different degrees of polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "degree = np.arange(0, 10)  # We'll try up to a 10th degree polynomial\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y, 'polynomialfeatures__degree', degree, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use grid search to find a good model; we can vary a couple of other hyperparameters too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'polynomialfeatures__degree': np.arange(10),\n",
    "    'linearregression__fit_intercept': [True, False],\n",
    "    'linearregression__normalize': [True, False]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Model Predictions with Ensembles\n",
    "\n",
    "One way to improve performance is to combine the results of several models. There are several ways this can be done; for example:\n",
    "\n",
    "* *Bagging* - building multiple models (typically of the same type) from different subsamples of the training dataset. Sampling is done with replacement, and the predictions are combined and averaged into a final result.\n",
    "* *Boosting* - building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n",
    "* *Voting* - building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions. *Stacking* is a voting approach which also learns the weights to give to each vote.\n",
    "\n",
    "If we have a model type in `model`, we can do bagging with:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "model = ...\n",
    "model = BaggingClassifier(base_estimator=model, n_estimators=50, random_state=0)\n",
    "...\n",
    "```\n",
    "\n",
    "Random forests are another example of bagging. AdaBoost is a common example of a boosting algorithm.\n",
    "\n",
    "\n",
    "## Interpreting Models\n",
    "\n",
    "Once we have a good model, how do we interpret the results? That is, how can we justify the prediction when it is questioned?\n",
    "\n",
    "For decision trees, this is not that difficult - the path through the decision tree illustrates the \"reasoning\" behind the decision. However, for many other models there is no intuitive interpretation. Getting a ranking of feature importance is helpful but not complete.\n",
    "\n",
    "There has been some interesting work in this area for classification models. The approach is to rely on a form of sensitivity analysis - ask, \"what small changes in the inputs would result in different outputs?\". This can be done one a case by case basis, with the learnings often being generalizable to cover more cases.\n",
    "\n",
    "This is really out of scope for this notebook, but if you are interested in learning more, the seminal work is LIME, which is described in this paper: https://arxiv.org/abs/1602.04938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
