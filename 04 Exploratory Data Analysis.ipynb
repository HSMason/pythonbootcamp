{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with Pandas\n",
    "\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to use the Python requests library which is great for accessing the web.\n",
    "!pip install requests\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video tutorials: http://www.dataschool.io/easier-data-analysis-with-pandas/\n",
    "\n",
    "A lot of the content of this notebook is based on Jake VanderPlas' excellent Python Data Science Handbook: https://jakevdp.github.io/PythonDataScienceHandbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy - the Foundation of Data Science in Python\n",
    "\n",
    "Data science is largely about the efficient manipulation of collections of numbers, so to support effective data science a language needs a way to do this. In Python this is done through the NumPy libary, which are much more memory- and computation-efficient than the built-in lists in Python.\n",
    "\n",
    "Python dicts are suboptimal for several reasons:\n",
    "\n",
    "* they are heterogeneous\n",
    "* even when storing numbers, these numbers are objects (as Python is a pure OOPL), and so are more than just (say) int32 or int64's like in C/C++; they have reference counts, type info, size info, and the actual data\n",
    "\n",
    "Python offers an array type which is homogeneous which improves om lists as far as storage goes, but offers limited operations on that data.\n",
    "\n",
    "NumPy bridges the gap, offering both efficient storage of homogeneous data in single or multi-dimensional arrays, and a rich set of operations on that data.\n",
    "\n",
    "In this section we will cover some of the basics of NumPy, but our focus will be mostly on Pandas, a library built on top of NumPy that is particularly well-suited to manipulating tabular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create one dimensional NumPy array from a list\n",
    "a = np.array([1, 2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append a value\n",
    "b = a\n",
    "a = np.append(a, 4)  # Note that this makes a copy; the original array is not affected\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the shape and # of elements\n",
    "print(np.shape(a))\n",
    "print(np.size(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index and slice\n",
    "print(f'Second element of a is {a[1]}')\n",
    "print(f'Last element of a is {a[-1]}')\n",
    "print(f'Middle two elements of a are {a[1:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a 2-D array from a list of lists\n",
    "b = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the shape, # of elements, and # of dimensions\n",
    "print(np.shape(b))\n",
    "print(np.size(b))\n",
    "print(np.ndim(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the first row of b; these are equivalent\n",
    "print(b[0]) \n",
    "print(b[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the first column of b\n",
    "print(b[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a subsection of b, from 1,1 through 2,2 (i.e. before 3,3)\n",
    "print(b[1:3,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array of zeros of length n\n",
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array of 1s\n",
    "np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creat an array of 10 random integers between 1 and 100\n",
    "np.random.randint(1,100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create linearly spaced array of 5 values from 0 to 100\n",
    "np.linspace(0, 100, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UFuncs\n",
    "\n",
    "NumPy supports highly efficient operations on arrays called UFuncs (Universal Functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(b)  # Get the mean of all the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.power(b, 2)  # Raise every element to second power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the details on UFuncs here: https://docs.scipy.org/doc/numpy-1.13.0/reference/ufuncs.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Series and Dataframes\n",
    "\n",
    "A Pandas Series is a one-dimensional array of indexed data. It wraps a sequence of values and a sequence of indices. The values are a NumPy array, while the indices are an instance of a pd.Index object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.Series([1, 4, 9, 16, 25])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal indexing and slicing operations are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where NumPy arrays have implicit integer sequence indices, Pandas indices are explicit and need not be integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.Series([1, 4, 9, 16, 25], index=['square of 1', 'square of 2', 'square of 3', 'square of 4', 'square of 5'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a Series is a lot like a Python dict (with additional slicing), and we can construct one from a Python dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series({'square of 1':1, 'square of 2':4, 'square of 3':9, 'square of 4':16, 'square of 5':25})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of dict-style operations work on a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'square of 5' in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.items()  # Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.values()  # Unlike Python dict, this is not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 6'] = 36  # We can add new entries\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 6'] = -1  # And change existing values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['square of 6']  # And delete a value\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series also behaves a lot like a list. We saw some indexing and slicing earlier. This can be done on non-numeric indexes too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 2': 'square of 4']  # This can be confusing as it INCLUDES the final value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 2': 'cube of 4']  # Be aware - a missing key will result in empty results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame is like a dictionary that maps column names to Series objects that share the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = pd.Series(['Alice', 'Bob', 'Carol'])\n",
    "phones = pd.Series(['555-123-4567', '555-987-6543', '555-245-6789'])\n",
    "\n",
    "df = pd.DataFrame({'Name': names, 'Phone': phones})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.index  # Like Series, DataFrame has an index for rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns  # DataFrame also has an index for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Name']  # Acts similar to dictionary; returns Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Name  # You can also access columns like this. Occasionally breaks if there is a name conflcit, like 'count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can add new columns. Later we'll see how to do this as a function of existing columns\n",
    "df['Closed'] = True\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to construct a DataFrame. For example, from a Series or dictionary of Series, from a list of Python dicts, or from a 2-D NumPy array. There are also utility functions to read data from disk into a DataFrame, e.g. from a .csv file or an Excel spreadsheet. We'll cover some of these later.\n",
    "\n",
    "Many DataFrame operations take an `axis` argument which defaults to zero. This specifies whether we want to apply the operation by rows (axis=0) or by columns (axis=1).\n",
    "\n",
    "## Indexes\n",
    "\n",
    "The Pandas Index can be thought of as an immutable ordered multiset (multiset as indices need not be unique). The immutability makes it safe to share an index between multiple columns of a DataFrame. The set-like properties are useful for things like joins (a join is like an intersection between Indexes). Let's look at some example operations to get more familiar with how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create two Indexes for experimentation\n",
    "\n",
    "i1 = pd.Index([1, 3, 5, 7, 9])\n",
    "i2 = pd.Index([2, 3, 5, 7, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1[2]  # We can index like an array with []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1[2:5]  # And slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1 & i2  # Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1 | i2  # Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1 ^ i2  # Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series and DataFrames have an explicit Index but they also have an implicit index like a list. If the Index uses integer values things can get confusing. In such cases it is good to be explicit; there are attributes for this:\n",
    "\n",
    "- `.loc` references the explicit Index\n",
    "- `.iloc` references the implicit Index\n",
    "\n",
    "The Python way is \"explicit is better than implicit\" so when indexing/slicing it is better to use these. The example below illustrates the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: explicit index starts at 1; implicit index starts at 0\n",
    "s = pd.Series(['first', 'second', 'third', 'fourth'], index=[1, 2, 3, 4]) \n",
    "\n",
    "print(f'Item at explicit index 1 is {s.loc[1]}')\n",
    "print(f'Item at implicit index 1 is {s.iloc[1]}')\n",
    "print(s.loc[1:3])\n",
    "print(s.iloc[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a CSV into a Dataframe\n",
    "\n",
    "Use `Pandas.read_csv` to read a CSV file into a dataframe. There are many optional argumemts that you can provide, for example to set or override column headers, skip initial rows, treat first row as containing column headers, specify the type of columns (Pandas will try to infer these otherwise), skip columns, and so on. The `parse_dates` argument is especially useful for specifying which columns have date fields as Pandas doesn't infer these.\n",
    "\n",
    "Full docs are at https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://samplecsvs.s3.amazonaws.com/SacramentocrimeJanuary2006.csv',\n",
    "                 parse_dates=['cdatetime'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Spreadsheet into a DataFrame\n",
    "\n",
    "Use Pandas.read_excel to load spreadsheet data. Full details here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Dataframe to a CSV or Excel spreadsheet\n",
    "\n",
    "You can use DataFrame.to_csv to write a DataFrame to a csv, and DataFrame.to_excel to save as a spreadsheet.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "# Get some sample data\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A Boolean expression on a Series will return a Series of Booleans\n",
    "titanic.survived == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you index a Series with a Boolean Series, you will select the items where the index is True.\n",
    "# So:\n",
    "titanic[titanic.survived == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can combine these with & and | for and and or\n",
    "# Pandas must use these normally bitwise operators because Python allows them to be overloaded\n",
    "# while 'and' and 'or' cannot be.\n",
    "# Unfortunately as these have higher operator precedence than relational operators, the \n",
    "# subexpressions we use with them need to be enclosed in parentheses.\n",
    "\n",
    "titanic[titanic.survived & (titanic.sex == 'female') & (titanic.age > 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "\n",
    "`pandas.concat` can be used to concatenate Series and DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = pd.Series(['A', 'B', 'C'])\n",
    "s2 = pd.Series(['D', 'E', 'F'])\n",
    "pd.concat([s1, s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Indexes are concatenated too. If you are using a simple row number index and don't want this behavior use the `ignore_index` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([s1, s2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can use `verify_integrity=True` to cause an exception to be raided if the result would have duplicate indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([s1, s2], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame([['A1', 'B1'],['A2', 'B2']], columns=['A', 'B'])\n",
    "d2 = pd.DataFrame([['C3', 'D3'],['C4', 'D4']], columns=['A', 'B'])\n",
    "d3 = pd.DataFrame([['B1', 'C1'],['B2', 'C2']], columns=['B', 'C'])\n",
    "pd.concat([d1, d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can join on other axis too.\n",
    "pd.concat([d1, d2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([d1, d3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the columns are not completely shared, additional NaN entries will be made.\n",
    "pd.concat([d1, d3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can force concat to only include the columns that are shared with an inner join.\n",
    "pd.concat([d1, d3], join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html for more options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Joining\n",
    "\n",
    "Pandas has a `merge` function that supports one-to-one, many-to-one and many-to-many joins. merge will look for matching column names between the inputs and use this as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame({'city': ['Seattle', 'Boston', 'New York'], 'population': [704352, 673184, 8537673]})\n",
    "d2 = pd.DataFrame({'city': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]})\n",
    "pd.merge(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can explicitly specify the column to join on; this is equivalent to the above example:\n",
    "pd.merge(d1, d2, on='city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the column names don't match you can specify the names to use:\n",
    "d3 = pd.DataFrame({'place': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]})\n",
    "pd.merge(d1, d3, left_on='city', right_on='place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want to drop the redundant column:\n",
    "pd.merge(d1, d3, left_on='city', right_on='place').drop('place', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merge` joins on arbitrary columns; if you want to join on the index you can use `left_index` and `right_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(list('ABC'), columns=['c1'])\n",
    "df2 = pd.DataFrame(list('DEF'), columns=['c2'])\n",
    "pd.merge(df1, df2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides a utility method on DataFrame, `join`, to do the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.join(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merge` can take a `how` argument that can be `inner`, `outer`, `left` or `right` to control the type of join. `inner` joins are the default.\n",
    "\n",
    "For more info on merging see https://pandas.pydata.org/pandas-docs/stable/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use unique() to see the full set of distinct values in a series\n",
    "titanic.deck.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# describe() will give summary statistics on a DataFrame. We first drop rows with NAs.\n",
    "titanic.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby('sex')['survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(['sex', 'class'])['survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame result is an example of a multi-indexed DataFrame (indexed by both 'sex' and 'class'). We're mostly going to ignore those in this notebook, but it is worth noting that Pandas has an `unstack` method that can turn a mutiply-indexed DataFrame back into a conventionally-indexed one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(['sex', 'class'])['survived'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All of the above can be achieved with a convenience pivot table method\n",
    "titanic.pivot_table('survived', index='sex', columns='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's break things down further by age group\n",
    "age = pd.cut(titanic['age'], [0, 18, 80])\n",
    "titanic.pivot_table('survived', index=['sex', age], columns='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index and colummns are also the second and third positional arguments, so we could just use:\n",
    "titanic.pivot_table('survived', ['sex', age], 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions\n",
    "\n",
    "We saw earlier that we can add new columns to a DataFrame easily. The new column can be a function of an existing column. For example, we could add an 'is_adult' field to the Titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['is_adult'] = titanic.age >= 18\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a simple case; we can do more complex row-by-row applications of arbitrary functions; here's the same change done differently (this would be much less efficient but may be the only option if the function is complex):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['is_adult'] = titanic.apply(lambda row: row['age'] >= 18, axis=1)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Operations\n",
    "\n",
    "Pandas has vectorized string operations that will skip over missing values. You can read about them here; we wil show a few examples: https://pandas.pydata.org/pandas-docs/stable/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the more detailed Titanic data set\n",
    "df = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper-case the home.dest field\n",
    "df['home.dest'].str.upper().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the field up into two\n",
    "place_df = df['home.dest'].str.split('/', expand=True)  # Expands the split list into DF columns\n",
    "place_df.columns = ['home', 'dest', '']  # For some reason there is a third column\n",
    "df['home'] = place_df['home']\n",
    "df['dest'] = place_df['dest']\n",
    "df = df.drop(['home.dest'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "To see if there are missing values, we can use isnull() to get a DataFrame showing the rows that have nulls, and where they have them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will show us the first few rows that had null values. If we want to know which columns may have nulls, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop rows that have missing values, use dropna(); add `inplace=True` to do it in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there are none - no-one could both be on a boat and be a recovered body, so at least one of these fields is always NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting\n",
    "\n",
    "See the Python Graph Gallery at https://python-graph-gallery.com/ for many examples of different types of charts including the code used to create them.\n",
    "\n",
    "There are many plotting libraries for Python; the most well known are matplotlib, seaborn (which extends matplotlib), Bokeh, and Plotly. Some offer more interactivity than others. We will use Seaborn for our examples. We first need to use the following magic to get the plots to show up in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get the more detailed Titanic data set\n",
    "df = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can use a factorplot to count categorical data\n",
    "import seaborn as sns\n",
    "sns.factorplot('sex',data=df,kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's bring class in too:\n",
    "sns.factorplot('pclass', data=df, hue='sex', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Of course we can aggregate the other way too\n",
    "sns.factorplot('sex', data=df, hue='pclass', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how many people were on each deck\n",
    "deck = pd.DataFrame(df['cabin'].dropna().str[0])\n",
    "deck.columns = ['deck']\n",
    "sns.factorplot('deck', data=deck, kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What class passenger was on each deck?\n",
    "df2 = df[['cabin', 'pclass']]\n",
    "df2 = df2.dropna()\n",
    "df2['deck'] = df2.apply(lambda row: ord(row.cabin[0]) -64, axis=1)\n",
    "\n",
    "sns.regplot(x=df2[\"pclass\"], y=df2[\"deck\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Data with pandas_profiling and facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "import seaborn as sns;\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "pandas_profiling.ProfileReport(titanic)  # You may need to run cell twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facets is a new library from Google that looks very good. It has similr functionality to pandas_profiling as well as some powerful visualization. Installation is more complex so we won't use it now but it is worth considering.\n",
    "\n",
    "https://github.com/pair-code/facets\n",
    "\n",
    "\n",
    "## Sample: Loading JSON into a DataFrame and Expanding Complex Fields\n",
    "\n",
    "In this example we'll see how we can load som structured data and process it into a flat table form better suited to machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get some data; top stories from lobste.rs; populate a DataFrame with the JSON\n",
    "stories = pd.read_json('https://lobste.rs/hottest.json')\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the \"short_id' field as the index\n",
    "stories = stories.set_index('short_id')\n",
    "\n",
    "# Show the first few rows\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the submitter_user field; it is a dictionary itself.\n",
    "stories.submitter_user[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to expand these fields into our dataframe. First expand into its own dataframe.\n",
    "user_df = stories.submitter_user.apply(pd.Series)\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We should make sure there are no collisions in column names.\n",
    "set(user_df.columns).intersection(stories.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can rename the column to avoid the clash\n",
    "user_df = user_df.rename(columns={'created_at': 'user_created_at'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now combine them, dropping the original compound column that we are expanding.\n",
    "stories = pd.concat([stories.drop(['submitter_user'], axis=1), user_df], axis=1)\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The tags field is another compound field.\n",
    "stories.tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a new dataframe with the tag lists expanded into columns of Series.\n",
    "tag_df = stories.tags.apply(pd.Series)\n",
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pivot the DataFrame\n",
    "tag_df = tag_df.stack()\n",
    "tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expand into a 1-hot encoding\n",
    "tag_df = pd.get_dummies(tag_df)\n",
    "tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge multiple rows\n",
    "tag_df = tag_df.sum(level=0)\n",
    "tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And add back to the original dataframe\n",
    "stories = pd.concat([stories.drop('tags', axis=1), tag_df], axis=1)\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 2: Baby Names\n",
    "\n",
    "In this sample some of the tasks will be left as exercises for you. The data comes from US census and is the count of names of children born in years from 1880 to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('NationalNames.csv.zip', compression='zip')  # Pandas can unzip the data for you\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: show the baby names from 1918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: get the counts per year for the name 'John'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: do the same but restrict to boys now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: plot popularity of John per year (hint: look at help for Seaborn barplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
