{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with NumPy and Pandas\n",
    "\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important imports. The \"as\" aliases are np and pd by convention.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video tutorials: http://www.dataschool.io/easier-data-analysis-with-pandas/\n",
    "\n",
    "Jake VanderPlas' excellent Python Data Science Handbook: https://jakevdp.github.io/PythonDataScienceHandbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy - the Foundation of Data Science in Python\n",
    "\n",
    "Data science is largely about the manipulation of (often large) collections of numbers, so to support effective data science a language needs a way to do this efficiently. In Python this is done through the NumPy libary, which provides much more memory- and computation-efficient ways than the built-in lists in Python.\n",
    "\n",
    "Python lists are suboptimal because they are heterogeneous collections of object references; the objects in turn have \n",
    "reference counts for garbage collection, type info, size info, and the actual data. Thus storing (say) a list of a four 32-bit integers, rather than requiring just 16 bytes requires much more.\n",
    "\n",
    "Python does offer an `array` type which is homogeneous and improves on lists as far as storage goes, but it offers limited operations on that data.\n",
    "\n",
    "NumPy bridges the gap, offering both efficient storage of homogeneous data in single or multi-dimensional arrays, and a rich set of operations on that data.\n",
    "\n",
    "In this section we will cover some of the basics of NumPy, but our focus will be mostly on Pandas, a library built on top of NumPy that is particularly well-suited to manipulating tabular data. You can get a deeper intro to NumPy here: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-dimensional NumPy array from a range\n",
    "a = np.arange(1, 11)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-dimensional NumPy array from a range with a specified increment\n",
    "a = np.arange(0.5, 10.5, 0.5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the array into a 4x5 matrix\n",
    "a = a.reshape(4, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create one dimensional NumPy array from a list\n",
    "a = np.array([1, 2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append a value\n",
    "b = a\n",
    "a = np.append(a, 4)  # Note that this makes a copy; the original array is not affected\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the shape and # of elements\n",
    "print(np.shape(a))\n",
    "print(np.size(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index and slice\n",
    "print(f'Second element of a is {a[1]}')\n",
    "print(f'Last element of a is {a[-1]}')\n",
    "print(f'Middle two elements of a are {a[1:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array of zeros of length n\n",
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array of 1s\n",
    "np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array of 10 random integers between 1 and 100\n",
    "np.random.randint(1,100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create linearly spaced array of 5 values from 0 to 100\n",
    "np.linspace(0, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2-D array from a list of lists\n",
    "b = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the shape, # of elements, and # of dimensions\n",
    "print(np.shape(b))\n",
    "print(np.size(b))\n",
    "print(np.ndim(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the first row of b; these are equivalent\n",
    "print(b[0]) \n",
    "print(b[0,:])  # First row, \"all columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the first column of b\n",
    "print(b[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a subsection of b, from 1,1 through 2,2 (i.e. before 3,3)\n",
    "print(b[1:3,1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy supports Boolean operations on arrays and using arrays of Boolean values to select elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an array of Booleans based on whether entries are odd or even numbers\n",
    "b%2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Boolean indexing to set all even values to -1\n",
    "b[b%2 == 0] = -1\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UFuncs\n",
    "\n",
    "NumPy supports highly efficient operations on arrays called UFuncs (Universal Functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(b)  # Get the mean of all the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.power(b, 2)  # Raise every element to second power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the details on UFuncs here: https://docs.scipy.org/doc/numpy-1.13.0/reference/ufuncs.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Series\n",
    "\n",
    "NumPy is primarily aimed at scientific computation e.g. linear algebra. As such, 2D data is in the form of arrays of arrays. In data science applications, we are more often dealing with tabular data; that is, collections of records (samples, observations) where each record may be heterogenous but the schema is consistent from record to record. The Pandas library is built on top of NumPy to provide this type of representation of data, along with the types of operations more typical in data science applications, like indexing, filtering and aggregation. There are two primary classes it provides for this, Series and DataFrame. \n",
    "\n",
    "A Pandas Series is a one-dimensional array of indexed data. It wraps a sequence of values and a sequence of indices, along with a name. The values are a NumPy array, while the indices are an instance of a pd.Index object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.Series([1, 4, 9, 16, 25])\n",
    "print(data.name)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above you can see that by default, a series will have numeric indices assigned, as a sequential list starting from 0, much like a typical Python list or array. The default name for the series is `None`, and the type of the data is `int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can show the first few lines with `.head()`. The argument, if omitted, defaults to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data need not be numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([\"quick\", \"brown\", \"fox\"], name=\"Fox\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have assigned a name to the series, and note that the data type is now `object`.\n",
    "\n",
    "What if we combine integers and strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, \"quick\", \"brown\", \"fox\"], name=\"Fox\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have \"missing\" values using None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([\"quick\", None, \"fox\"], name=\"Fox\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a series of type `object`, `None` can simply be included, but what if the series is numeric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, None, 3])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the special float value *NaN* (`np.nan`) is used in this case. This is also why the series has type float64 and not int64; floating point numbers have special values for NaN while ints don't.\n",
    "\n",
    "Be careful with NaN; it will fail equality tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead you can use `is` or `np.isnan()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nan is np.nan)\n",
    "print(np.isnan(np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal indexing and slicing operations are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, 4, 9, 16, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where NumPy arrays have implicit integer sequence indices, Pandas indices are explicit and need not be integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.Series([1, 4, 9, 16, 25], index=['square of 1', 'square of 2', 'square of 3', 'square of 4', 'square of 5'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a Series is a lot like a Python dict (with additional slicing), and we can construct one from a Python dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series({'square of 1':1, 'square of 2':4, 'square of 3':9, 'square of 4':16, 'square of 5':25})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use both a dictionary and an explicit index but be careful if the index and dictionary keys don't align completely; the explicit index takes precedence. Look at what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({\"one\": 1, \"three\": 3}, index=[\"one\", \"two\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Given the list below, create a Series that has the list as both the index and the values, and then display the first 3 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex1 = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Put your code here.\n",
    "# Uncomment and run the %load magic for a sample solution.\n",
    "# %load ex04-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of dict-style operations work on a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruct the Series\n",
    "data = pd.Series([1, 4, 9, 16, 25], index=['square of 1', 'square of 2', 'square of 3', 'square of 4', 'square of 5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'square of 5' in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.items()  # Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.values  # Unlike Python dict, this is not the same - it's an array, not a function returning an interable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 6'] = 36  # We can add new entries\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 6'] = -1  # And change existing values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['square of 6']  # And delete a value\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration (`__iter__`) iterates over the values in a Series, while membership testing (`__contains__`) checks the indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in data:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(16 in data)\n",
    "print('square of 4' in data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Operations\n",
    "\n",
    "You can iterate over a Series or Dataframe, but in many cases there are much more efficient vectorized UFuncs available; these are implemented in native code exploiting parallel processor operations and are much faster. Some examples are `.sum()`, `.median()`, `.mode()`, and `.mean()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series also behaves a lot like a list. We saw some indexing and slicing earlier. This can be done on non-numeric indexes too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 2': 'square of 4']  # This can be confusing as it INCLUDES the final value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['square of 2': 'cube of 4']  # Be aware - a missing key will result in empty results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Delete the row 'k' from the earlier series, then display the rows from 'f' through 'l'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2: put your code here\n",
    "# %load ex04-02.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas DataFrames\n",
    "\n",
    "A DataFrame is like a dictionary of Series that share the same index, keyed on the Series names.\n",
    "\n",
    "Read the sentence above again and make sure it makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = pd.Series(['Alice', 'Bob', 'Carol'])\n",
    "phones = pd.Series(['555-123-4567', '555-987-6543', '555-245-6789'])\n",
    "dept = pd.Series(['Marketing', 'Accounts', 'HR'])\n",
    "\n",
    "df = pd.DataFrame({'Name': names, 'Phone': phones, 'Department': dept})  # 'Name', 'Phone', 'Department' are the column names\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.index  # Like Series, DataFrame has an index for rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns  # DataFrame also has an index for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index operator actually selects a column in the DataFrame, while the .iloc and .loc attributes still select rows (actually, we will see in the next section that they can select a subset of the DataFrame with a row selector and column selector, but the row selector comes first so if you supply a single argument to .loc or .iloc you will select rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Name']  # Acts similar to dictionary; returns Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a transpose of the DataFrame with the .T attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Name  # You can also access columns like this, with dot-notation.\n",
    "# Occasionally this breaks if there is a name conflcit with a UFunc, like 'count'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can add new columns. Later we'll see how to do this as a function of existing columns\n",
    "df['Closed'] = True\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use .describe() to get summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use .drop() to remove rows. Makes a copy unless you include 'inplace=True'.\n",
    "df.drop([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that because we didn't say inplace=True,\n",
    "# the original is unchanged\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to construct a DataFrame. For example, from a Series or dictionary of Series, from a list of Python dicts, or from a 2-D NumPy array. There are also utility functions to read data from disk into a DataFrame, e.g. from a .csv file or an Excel spreadsheet. We'll cover some of these later.\n",
    "\n",
    "Many DataFrame operations take an `axis` argument which defaults to zero. This specifies whether we want to apply the operation by rows (axis=0) or by columns (axis=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can drop columns if you specify axis=1\n",
    "df.drop([\"Name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A way to remove a column inplace is to use del\n",
    "del df[\"Department\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change the index to be some other column.\n",
    "# If we want to save the existing index, we first\n",
    "# need to add it as a new column:\n",
    "\n",
    "df['Number'] = df.index\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can set the new index. This is a destruvtive\n",
    "# operation that discards the old index, which is\n",
    "# why we saved it as a new column first.\n",
    "df = df.set_index('Name')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively we can promote the index to a column and go \n",
    "# back to a numeric index with reset_index()\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Create a DataFrame from the dictionary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n",
    "        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n",
    "        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n",
    "        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment and run next line for solutions\n",
    "# %load ex04-03.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code to create the DataFrame here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a summary of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the sum of all visits (the total number of visits).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Indexing\n",
    "\n",
    "The Pandas Index type can be thought of as an immutable ordered multiset (multiset as indices need not be unique). The immutability makes it safe to share an index between multiple columns of a DataFrame. The set-like properties are useful for things like joins (a join is like an intersection between Indexes). There are dict-like properties (index by label) and list-like properties too (index by location). \n",
    "\n",
    "Indexes are complicated but understanding them is key to leveraging the power of pandas. Let's look at some example operations to get more familiar with how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create two Indexes for experimentation\n",
    "\n",
    "i1 = pd.Index([1, 3, 5, 7, 9])\n",
    "i2 = pd.Index([2, 3, 5, 7, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1[2]  # We can index like a list with []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1[2:5]  # And slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1 & i2  # Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1 | i2  # Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1 ^ i2  # Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series and DataFrames have an explicit Index but they also have an implicit index like a list. When using the `[]` operator, the type of the argument will determine which index is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1, 2], index=[\"1\", \"2\"])\n",
    "print(s[\"1\"])  # matches index type; use explicit\n",
    "print(s[1])  # integer doesn't match index type; use implicit positional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the explicit Index uses integer values things can get confusing. In such cases it is good to make your intent explicit; there are attributes for this:\n",
    "\n",
    "- `.loc` references the explicit Index\n",
    "- `.iloc` references the implicit Index; i.e. a positional index 0, 1, 2,...\n",
    "\n",
    "The Python way is \"explicit is better than implicit\" so when indexing/slicing it is better to use these. The example below illustrates the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: explicit index starts at 1; implicit index starts at 0\n",
    "s = pd.Series(['first', 'second', 'third', 'fourth'], index=[1, 2, 3, 4]) \n",
    "\n",
    "print(f'Item at explicit index 1 is {s.loc[1]}')\n",
    "print(f'Item at implicit index 1 is {s.iloc[1]}')\n",
    "print(s.loc[1:3])\n",
    "print(s.iloc[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `.iloc`, the expression in `[]` can be:\n",
    "\n",
    "* an integer, a list of integers, or a slice object (e.g. `1:7`)\n",
    "* a Boolean array (see Filtering section below for why this is very useful)\n",
    "* a function with one argument (the calling object) that returns one of the above\n",
    "\n",
    "Selecing outside of the bounds of the object will raise an IndexError except when using slicing.\n",
    "\n",
    "When using `.loc`, the expression in `[]` can be:\n",
    "\n",
    "* an label, a list of labels, or a slice object with labels (e.g. `'a':'f'`; unlike normal slices the stop label is included in the slice)\n",
    "* a Boolean array\n",
    "* a function with one argument (the calling object) that returns one of the above\n",
    "\n",
    "You can use one or two dimensions in `[]` after `.loc` or `.iloc` depending on whether you want to select a subset of rows, columns, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `set_index` method to change the index of a DataFrame.\n",
    "\n",
    "If you want to change entries in a DataFrame selectively to some other value, you can use assignment with indexing, such as:\n",
    "\n",
    "```python\n",
    "df.loc[row_indexer, column_indexer] = value\n",
    "```\n",
    "\n",
    "_Don't_ use:\n",
    "\n",
    "```python\n",
    "df[row_indexer][column_indexer] = value\n",
    "```\n",
    "\n",
    "That _chained indexing_ can result in copies being made which will not have the effect you expect. You want to do all your indexing in one operation. See the details at https://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
    "    \n",
    " ## Exercise 4\n",
    " \n",
    " Use the same DataFrame from Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment and run %load for solutions\n",
    "# %load ex04-04.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select just the 'animal' and 'age' columns from the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the data in rows [3, 5, 7] and in columns ['animal', 'age'].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading/Saving CSV, JSON and Excel Files\n",
    "\n",
    "Use `Pandas.read_csv` to read a CSV file into a dataframe. There are many optional argumemts that you can provide, for example to set or override column headers, skip initial rows, treat first row as containing column headers, specify the type of columns (Pandas will try to infer these otherwise), skip columns, and so on. The `parse_dates` argument is especially useful for specifying which columns have date fields as Pandas doesn't infer these.\n",
    "\n",
    "Full docs are at https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://samplecsvs.s3.amazonaws.com/SacramentocrimeJanuary2006.csv',\n",
    "                 parse_dates=['cdatetime'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to do some preprocessing of a field during loading you can use the `converters` argument which takes a dictionary mapping the field names to lambda functions that munge the field. E.g. if you had a field `zip` and you wanted to take just the first 3 digits, you could use:\n",
    "\n",
    "```python\n",
    "..., converters={'zip': lambda x: x[:3]}, ...\n",
    "```\n",
    "\n",
    "You can pass a dictionary in with the `types` argument that maps field names to NumPy types, to override the type inference. You can see details of NumPy scalar types here: https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html\n",
    "\n",
    "By default the first line is expected to contain the column headers. If it doesn't you can specify them yourself, using arguments such as:\n",
    "\n",
    "```python\n",
    "..., header=None, names=['column1name','column2name'], ...\n",
    "```\n",
    "\n",
    "If the separator is not a comma, use the `sep` argument; e.g. for a TAB-separated file:\n",
    "\n",
    "```python\n",
    "..., sep='\\t', ...\n",
    "```\n",
    "\n",
    "Use `Pandas.read_excel` to load spreadsheet data. Full details here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `DataFrame.to_csv` method to save a DataFrame to a file or `DataFrame.to_excel` to save as a spreadsheet.\n",
    "\n",
    "It's also possible to read JSON data into a DataFrame. The complexity here is that JSON data is typically hierarchical; in order to turn it into a DataFrame the data typically needs to be flattened in some way. This iss controlled by an `orient` parameter. For details see https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "You can sort a DataFrame using the `sort_values` method:\n",
    "\n",
    "```python\n",
    "DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, na_position='last')\n",
    "```\n",
    "    \n",
    "The `by` argument should be a column name or list of column names in priority order (if axis=0, i.e. we are sorting the rows, which is typically the case).\n",
    "\n",
    "See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html for the details.\n",
    "    \n",
    "\n",
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "# Get some sample data\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A Boolean expression on a Series will return a Series of Booleans\n",
    "titanic.survived == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you index a Series with a Boolean Series, you will select the items where the index is True.\n",
    "# So:\n",
    "titanic[titanic.survived == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can combine these with & and | for and and or\n",
    "# Pandas must use these normally bitwise operators because Python allows them to be overloaded\n",
    "# while 'and' and 'or' cannot be, and in any event they arguably make sense as they are operating\n",
    "# on Boolean series which are similar to bit vectors.\n",
    "# Unfortunately as these have higher operator precedence than relational operators, the \n",
    "# subexpressions we use with them need to be enclosed in parentheses.\n",
    "\n",
    "titanic[titanic.survived & (titanic.sex == 'female') & (titanic.age > 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy itself also supports such Boolean filtering; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([3, 2, 4, 1, 5])\n",
    "s[s > np.mean(s)]  # Get the values above the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "To see if there are missing values, we can use isnull() to get a DataFrame showing the rows that have nulls, and where they have them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.isnull().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will show us the first few rows that had null values. If we want to know which columns may have nulls, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop rows that have missing values, use dropna(); add `inplace=True` to do it in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there are none - no-one could both be on a boat and be a recovered body, so at least one of these fields is always NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD - cover fillna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Using the previous DataFrame from exercise 3, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For solutions, uncomment %load and execute.\n",
    "# %load ex04-05.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select only the rows where the number of visits is greater than or equal to 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the rows where the age is missing, i.e. is NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the rows where the animal is a cat and the age is less than 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the rows the age is between 2 and 4 (inclusive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the index to use this list:\n",
    "# idx = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the age in row 'f' to 1.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append a new row 'k' to df with your choice of values for each column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then delete that row to return the original DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "\n",
    "`pandas.concat` can be used to concatenate Series and DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = pd.Series(['A', 'B', 'C'])\n",
    "s2 = pd.Series(['D', 'E', 'F'])\n",
    "df = pd.concat([s1, s2])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Indexes are concatenated too, so if you are using a simple row number index you can end up with duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you don't want this behavior use the `ignore_index` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([s1, s2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can use `verify_integrity=True` to cause an exception to be raised if the result would have duplicate indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([s1, s2], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame([['A1', 'B1'],['A2', 'B2']], columns=['A', 'B'])\n",
    "d2 = pd.DataFrame([['C3', 'D3'],['C4', 'D4']], columns=['A', 'B'])\n",
    "d3 = pd.DataFrame([['B1', 'C1'],['B2', 'C2']], columns=['B', 'C'])\n",
    "pd.concat([d1, d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can join on other axis too.\n",
    "pd.concat([d1, d2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([d1, d3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the columns are not completely shared, additional NaN entries will be made.\n",
    "pd.concat([d1, d3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can force concat to only include the columns that are shared with an inner join.\n",
    "pd.concat([d1, d3], join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html for more options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Joining\n",
    "\n",
    "We have already seen how we can add a new column to a DataFrame when it is a fixed scalar value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(['Fred', 'Alice', 'Joe'], columns=['Name'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Married'] = False\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also give an array of values or provided it has the same length, or we can use a Series keyed on the index if it is not the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Phone'] = ['555-123-4567', '555-321-0000', '555-999-8765']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Department'] = pd.Series({0: 'HR', 2: 'Marketing'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to join two DataFrames instead. Pandas has a `merge` function that supports one-to-one, many-to-one and many-to-many joins. merge will look for matching column names between the inputs and use this as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame({'city': ['Seattle', 'Boston', 'New York'], 'population': [704352, 673184, 8537673]})\n",
    "d2 = pd.DataFrame({'city': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]})\n",
    "pd.merge(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can explicitly specify the column to join on; this is equivalent to the above example:\n",
    "pd.merge(d1, d2, on='city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the column names don't match you can specify the names to use:\n",
    "d3 = pd.DataFrame({'place': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]})\n",
    "pd.merge(d1, d3, left_on='city', right_on='place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want to drop the redundant column:\n",
    "pd.merge(d1, d3, left_on='city', right_on='place').drop('place', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merge` joins on arbitrary columns; if you want to join on the index you can use `left_index` and `right_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(list('ABC'), columns=['c1'])\n",
    "df2 = pd.DataFrame(list('DEF'), columns=['c2'])\n",
    "pd.merge(df1, df2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides a utility method on DataFrame, `join`, to do the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.join(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merge` can take a `how` argument that can be `inner` (intersection), `outer` (union), `left` (first augmented by second) or `right` (second augmented by first)  to control the type of join. `inner` joins are the default.\n",
    "\n",
    "If there are other columns with the same name between the two DataFrames, Pandas will give them unique names by appending `_x` to the columns from the first argument and `_y` to the columns from the second argument.\n",
    "\n",
    "It's also possible to use list of column names for the `left_on` and `right_on` arguments to join on multiple columns.\n",
    "\n",
    "For more info on merging see https://pandas.pydata.org/pandas-docs/stable/merging.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating and Pivot Tables\n",
    "\n",
    "TODO: add some more info here, and on split-apply-combine pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use unique() to see the full set of distinct values in a series\n",
    "titanic.deck.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use value_counts() to get the counts of the unique values\n",
    "titanic.deck.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# describe() will give summary statistics on a DataFrame. We first drop rows with NAs.\n",
    "titanic.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby('sex')['survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(['sex', 'class'])['survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame result is an example of a multi-indexed DataFrame (indexed by both 'sex' and 'class'). We're mostly going to ignore those in this notebook, but it is worth noting that Pandas has an `unstack` method that can turn a mutiply-indexed DataFrame back into a conventionally-indexed one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(['sex', 'class'])['survived'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All of the above can be achieved with a convenience pivot table method\n",
    "titanic.pivot_table('survived', index='sex', columns='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's break things down further by age group\n",
    "age = pd.cut(titanic['age'], [0, 18, 80])\n",
    "titanic.pivot_table('survived', index=['sex', age], columns='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index and colummns are also the second and third positional arguments, so we could just use:\n",
    "titanic.pivot_table('survived', ['sex', age], 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions\n",
    "\n",
    "We saw earlier that we can add new columns to a DataFrame easily. The new column can be a function of an existing column. For example, we could add an 'is_adult' field to the Titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['is_adult'] = titanic.age >= 18\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a simple case; we can do more complex row-by-row applications of arbitrary functions; here's the same change done differently (this would be much less efficient but may be the only option if the function is complex):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['is_adult'] = titanic.apply(lambda row: row['age'] >= 18, axis=1)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Use the same DataFrame from exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the mean age for each different type of animal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count the number of each type of animal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the data first by the values in the 'age' column in decending order,\n",
    "# then by the value in the 'visits' column in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the 'animal' column, change the 'snake' entries to 'python'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The 'priority' column contains the values 'yes' and 'no'. Replace this column with a column of boolean values: \n",
    "#'yes' should be True and 'no' should be False.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Operations\n",
    "\n",
    "Pandas has vectorized string operations that will skip over missing values. You can read about them here; we wil show a few examples: https://pandas.pydata.org/pandas-docs/stable/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get the more detailed Titanic data set\n",
    "df = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upper-case the home.dest field\n",
    "df['home.dest'].str.upper().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's split the field up into two\n",
    "place_df = df['home.dest'].str.split('/', expand=True)  # Expands the split list into DF columns\n",
    "place_df.columns = ['home', 'dest', '']  # For some reason there is a third column\n",
    "df['home'] = place_df['home']\n",
    "df['dest'] = place_df['dest']\n",
    "df = df.drop(['home.dest'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal and Categorical Data\n",
    "\n",
    "TBD\n",
    "\n",
    "## Working with Dates and Time Series\n",
    "\n",
    "TBD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligned Operations\n",
    "\n",
    "Pandas will align DataFrames on indexes when performing operations. Consider for example two DataFrames, one with number of transactions by day of week, and one with number of customers by day of week, and say we want to know average transactions per customer by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions = pd.DataFrame([2, 4, 5],\n",
    "                           index=['Mon', 'Wed', 'Thu'])\n",
    "customers = pd.DataFrame([2, 2, 3, 2], \n",
    "                        index=['Sat', 'Mon', 'Tue', 'Thu'])\n",
    "\n",
    "transactions / customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how pandas aligned on index to produce the result, and used NaN for mismatched entries. We could specify the value to use as operands by using the `div` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions.div(customers, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Methods and .pipe()\n",
    "\n",
    "Many operations on Series and DataFrames return copies, unless the `inplace=True` argument is included. Even in that case there is usually a copy made and then the reference is just replaced at the end; this means using inplace operations generally isn't faster. Because a modified copy is returned, you can chain multiple changes, for example:\n",
    "\n",
    "```python\n",
    "df = (pd.read_csv('data.csv')\n",
    "        .rename(columns=str.lower)\n",
    "        .drop('id', axis=1))\n",
    "```\n",
    "\n",
    "This is great for built-in operations, but what about custom operations? The good news is these are possible too, with `.pipe()`:\n",
    "\n",
    "```python\n",
    "def my_operation(df, *args, **kwargs):\n",
    "    # Do something to the df\n",
    "    ...\n",
    "    # Return the modified dataframe\n",
    "    return df\n",
    "   \n",
    "# Now we can call this in our chain.\n",
    "df = (pd.read_csv('data.csv')\n",
    "        .rename(columns=str.lower)\n",
    "        .drop('id', axis=1)\n",
    "        .pipe(my_operation, 'foo', bar=True))    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiIndexes\n",
    "\n",
    "TBD\n",
    "\n",
    "Note that we can transpose and get hierachical column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance and Hypothesis Testing\n",
    "\n",
    "TBD - extend this and talk about p-hacking\n",
    "\n",
    "In exploring the data, we may come up with hypotheses about relationships between different values. We can get an indication of whether our hypothesis is correct or the relationship is cincidental using tests of statistical significance.\n",
    "\n",
    "The usual approach is to assume the opposite of what we want to prove; this is called the *null hypothesis*. We then calculate the probability that the data supports the null hypothesis: this is called the *p-value*. In general, a p-value of less than 0.05 is taken to mean that the hypothesis is valid (although this has recently become a contentious point; see http://www.sciencemag.org/news/2017/07/it-will-be-much-harder-call-new-findings-significant-if-team-gets-its-way). We'll set aside that debate for now and stick with 0.05.\n",
    "\n",
    "Let's revisit the Titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how gender affected survival rates, one way is with cross-tabulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(titanic['survived'],titanic['sex'])\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a lot more men than women on the ship, and it certainly looks like the survival rate for women was better than for men, but is the difference statistcially significant? Our  hypothesis is that gender affects survivability, and so the null hypothesis is that it doesn't. One way to measure this is with a chi-squared test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "chi2, p, dof, expected = stats.chi2_contingency(ct.values)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a very small p-value! So we can be sure gender was an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Plots\n",
    "\n",
    "Pandas includes the ability to do simple plots. For a Series, this typically means plotting the values in the series as the Y values, and then index as the X values; for a DataFrame this would be a multiplot. You can use `x` and `y` named arguments to select specific columns to plot, and you can use a `kind` argument to specify the type of plot.\n",
    "\n",
    "See https://pandas.pydata.org/pandas-docs/stable/visualization.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series([2, 3, 1, 5, 3], index=['a', 'b', 'c', 'd', 'e'])\n",
    "s.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [2, 1],\n",
    "        [4, 4],\n",
    "        [1, 2],\n",
    "        [3, 6]\n",
    "    ],\n",
    "    index=['a', 'b', 'c', 'd'],\n",
    "    columns=['s1', 's2']\n",
    ")\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.plot(x='s1', y='s2', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting with Seaborn\n",
    "\n",
    "See the Python Graph Gallery at https://python-graph-gallery.com/ for many examples of different types of charts including the code used to create them.\n",
    "\n",
    "There are many plotting libraries for Python; the most well known are matplotlib, seaborn (which extends matplotlib), Bokeh, and Plotly. Some offer more interactivity than others. Seaborn is a popular library so we will examine it with some examples. We first need to use the following magic to get the plots to show up in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get the more detailed Titanic data set\n",
    "df = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can use a factorplot to count categorical data\n",
    "import seaborn as sns\n",
    "sns.factorplot('sex',data=df,kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's bring class in too:\n",
    "sns.factorplot('pclass', data=df, hue='sex', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Of course we can aggregate the other way too\n",
    "sns.factorplot('sex', data=df, hue='pclass', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how many people were on each deck\n",
    "deck = pd.DataFrame(df['cabin'].dropna().str[0])\n",
    "deck.columns = ['deck']\n",
    "sns.factorplot('deck', data=deck, kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What class passenger was on each deck?\n",
    "df2 = df[['cabin', 'pclass']]\n",
    "df2 = df2.dropna()\n",
    "df2['deck'] = df2.apply(lambda row: ord(row.cabin[0]) -64, axis=1)\n",
    "\n",
    "sns.regplot(x=df2[\"pclass\"], y=df2[\"deck\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Data with pandas_profiling and facets\n",
    "\n",
    "`pandas_profiling` is a Python package that can produce much more detailed summaries of data than the `.describe()` method. In this case we must install with `pip` and the right way to do this from the notebook is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas_profiling\n",
    "import seaborn as sns;\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "pandas_profiling.ProfileReport(titanic)  # You may need to run cell twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facets is a new library from Google that looks very good. It has similar functionality to pandas_profiling as well as some powerful visualization. Installation is more complex so we won't use it now but it is worth considering.\n",
    "\n",
    "https://github.com/pair-code/facets\n",
    "\n",
    "## Handling Data that Exceeds Your System's RAM\n",
    "\n",
    "Pandas is an in-memory system. The use of NumPy means it uses memory very efficiently but you are still limited by the RAM you have available. If your data is too large, there are several options available, incliuding:\n",
    "\n",
    "- partition the data into chunks and process them sequentially\n",
    "- partition the data into chunks and use multiple computers configured as a cluster with `ipyparallel` (https://ipyparallel.readthedocs.io/en/latest/)\n",
    "- use a DataFrame-like library that handles larger datasets, like Dask DataFrames (http://dask.pydata.org/en/latest/dataframe.html) \n",
    "- putting the data in a database and operating on a subset in Pandas using a SELECT statement.\n",
    "\n",
    "These are all out of scope of this document but we will briefly elaborate on the last two. Python comes standard with an implementation of SqlLite, in the package `sqllite3`. Pandas supports reading a DataFrame from the result of running a query against a SqlLite database. Here's a very simple example of how that may look:\n",
    "\n",
    "```python\n",
    "import sqlite3 as lite\n",
    "\n",
    "with lite.connect('mydata.db') as con:\n",
    "    query = 'select * from sales limit 100'\n",
    "    df = pd.read_sql(query, con)\n",
    "```\n",
    "\n",
    "You can read more about SqlLite here: https://sqlite.org/quickstart.html.\n",
    "\n",
    "Dask supports chunked dataframes that support most of the functionality of Pandas. The key additional parameter is `blocksize` which specifies the maximum size of a chunk of data to read into memory at one time. In addition, Dask methods are lazily evaluated; you must explicitly call a `.compute()` method to kick off the calculation. Here is a simple example: assume we have multiple CSV files containing temperature measurements. We could compute the mean temperature with something like:\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv('temp*.csv', blocksize=25e6)  # Use 25MB chunks\n",
    "df.temperature.mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Packages\n",
    "\n",
    "- `openpyxl` allows you to create and work directly with Excel spreadsheets\n",
    "- `faker` can create fake dfata like names, addresses, credit card numbers, and social security numbers\n",
    "- `numba` includes a `@jit` decorator that can speed up the execution of many functions; useful when crunching data outside of Pandas (it won't speed up Pandas code)\n",
    "- `moviepy` allows you to edit video frame-by-frame (or even create video)\n",
    "\n",
    "\n",
    "## Example: Loading JSON into a DataFrame and Expanding Complex Fields\n",
    "\n",
    "In this example we'll see how we can load some structured data and process it into a flat table form better suited to machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get some data; top stories from lobste.rs; populate a DataFrame with the JSON\n",
    "stories = pd.read_json('https://lobste.rs/hottest.json')\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the \"short_id' field as the index\n",
    "stories = stories.set_index('short_id')\n",
    "\n",
    "# Show the first few rows\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the submitter_user field; it is a dictionary itself.\n",
    "stories.submitter_user[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to expand these fields into our dataframe. First expand into its own dataframe.\n",
    "user_df = stories.submitter_user.apply(pd.Series)\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We should make sure there are no collisions in column names.\n",
    "set(user_df.columns).intersection(stories.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can rename the column to avoid the clash\n",
    "user_df = user_df.rename(columns={'created_at': 'user_created_at'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now combine them, dropping the original compound column that we are expanding.\n",
    "stories = pd.concat([stories.drop(['submitter_user'], axis=1), user_df], axis=1)\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The tags field is another compound field.\n",
    "stories.tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a new dataframe with the tag lists expanded into columns of Series.\n",
    "tag_df = stories.tags.apply(pd.Series)\n",
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pivot the DataFrame\n",
    "tag_df = tag_df.stack()\n",
    "tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expand into a 1-hot encoding\n",
    "tag_df = pd.get_dummies(tag_df)\n",
    "tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge multiple rows\n",
    "tag_df = tag_df.sum(level=0)\n",
    "tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And add back to the original dataframe\n",
    "stories = pd.concat([stories.drop('tags', axis=1), tag_df], axis=1)\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Baby Names\n",
    "\n",
    "The data comes from US census and is the count of names of children born in years from 1880 to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('NationalNames.csv.zip', compression='zip')  # Pandas can unzip the data for you\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: show the baby names from 1918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: get the counts per year for the name 'John'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: do the same but restrict to boys now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: plot popularity of John as a boy's name per year\n",
    "# (hint: look at help for Seaborn barplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
